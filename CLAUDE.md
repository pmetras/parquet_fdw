# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

parquet_fdw is a read-only PostgreSQL Foreign Data Wrapper (FDW) extension for Apache Parquet files. It allows querying Parquet files as if they were PostgreSQL tables.

**Languages**: C (PostgreSQL module entry) and C++ (core implementation)
**Requires**: C++17, libarrow 0.15+, libparquet, PostgreSQL 10-17

## Build Commands

```bash
# Build and install (requires pg_config in PATH)
make install

# Custom PostgreSQL installation
make install PG_CONFIG=/path/to/pg_config

# Run regression tests (requires running PostgreSQL instance with extension installed)
make installcheck

# Clean build artifacts
make clean
```

## Testing

Tests use PostgreSQL's `pg_regress` framework. Test files are in `test/sql/*.sql.in` (templates) with expected output in `test/expected/*.out.in`. The Makefile generates actual `.sql` and `.out` files by replacing `@abs_srcdir@` with the repository path.

Test data is generated by `test/data/generate.py` using PyArrow/Pandas. Test categories:
- `001_parquet_fdw.sql.in`: Basic operations, filtering, sorting
- `002_import.sql.in`: Schema import functionality
- `003_partition.sql.in`: Partitioned data handling

After running `make installcheck`, check `test/regression.diffs` for failures.

## Architecture

### Source Code Structure

- `src/parquet_fdw.c` - PostgreSQL module entry point, GUC variable definitions
- `src/parquet_impl.cpp` - Main FDW implementation: planning, path selection, schema import
- `src/reader.cpp` - Parquet file reader using Apache Arrow
- `src/exec_state.cpp` - Execution state management for different reader strategies
- `src/common.cpp` - Utilities: type conversion, error handling, file globbing

### Execution Strategies

The FDW selects a reader strategy based on file count and options:

| Reader Type | When Used |
|-------------|-----------|
| Single | One file |
| Multifile | Multiple files, sequential processing |
| Multifile Merge | Multiple presorted files with `sorted` option |
| Caching Multifile Merge | Merge when files exceed `max_open_files` |

### Type Mapping (Arrow → PostgreSQL)

INT8/INT16→INT2, INT32→INT4, INT64→INT8, UINT8→INT2, UINT16→INT4, UINT32→INT8, UINT64→NUMERIC, FLOAT→FLOAT4, DOUBLE→FLOAT8, TIMESTAMP→TIMESTAMP, DATE32→DATE, STRING/LARGE_STRING→TEXT, BINARY/LARGE_BINARY→BYTEA, LIST→ARRAY, MAP→JSONB, UUID→UUID, FIXED_SIZE_BINARY(16)→UUID

### Key Table Options

- `filename`: Space-separated paths (supports glob patterns and brace enumeration like `{a,b}`)
- `sorted`: Columns files are presorted by
- `files_in_order`: Files are ordered with no overlap (enables Gather Merge)
- `use_threads`: Enable Arrow's parallel decoding
- `max_open_files`: Limit simultaneous open files

### PostgreSQL/C++ Integration Pattern

The code uses `PG_TRY/PG_CATCH` blocks to translate C++ exceptions to PostgreSQL errors. Memory management uses PostgreSQL's MemoryContext system alongside C++ RAII patterns.

## GUC Variables

```sql
SET parquet_fdw.use_threads = true;          -- Enable/disable threading
SET parquet_fdw.enable_multifile = true;     -- Enable multifile reader
SET parquet_fdw.enable_multifile_merge = true; -- Enable merge reader
```

# The rules you MUST follow

- Ask the user when you need to run a command with root privileges (i.e. `sudo`).
- Never install Python system packages. Use the `.venv` environment present in the project directory.
- The project is developed on a computer with multiple instances of different PostgreSQL versions installed. PostgreSQL versions and libraries versions must be taken into account when building deliverables. Naming of deliverable must indicate which version is expected.
- Full test data is on a PostgreSQL server that is protected and can't be reached. Ask user for details if you need schema information.
